#!/usr/bin/env python3
"""
Script: preprocess_products.py
Descripci√≥n: Pre-procesa productos Sentinel-1 con PyroSAR: recorte al AOI y creaci√≥n de mosaicos
Uso: 
  python scripts/preprocess_products.py [--grd|--slc] [--insar-mode]

Modos de preprocesamiento:
  GRD: Read ‚Üí Subset
  SLC Normal: Read ‚Üí TOPSAR-Split ‚Üí TOPSAR-Deburst ‚Üí Subset
  SLC InSAR: Read ‚Üí TOPSAR-Split ‚Üí Subset (sin Deburst, mantiene estructura de bursts)

Este script:
1. Identifica productos Sentinel-1 descargados
2. Recorta productos al AOI usando PyroSAR
3. Aplica TOPSAR-Deburst solo en modo SAR normal (NO en modo InSAR)
4. Crea mosaicos de productos que cubren la misma fecha/√≥rbita
5. Guarda productos pre-procesados listos para procesamiento SAR/InSAR
"""

import os
import sys
import glob
import re
import logging
from datetime import datetime
import argparse
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed

# Predefinir nombres de m√≥dulos/imports para silenciar advertencias est√°ticas
pyroSAR = None
identify = None
geocode = None
shape = None
shapely_wkt = None
gpd = None

# PyroSAR es opcional
pyroSAR_available = False
try:
    import pyroSAR
    from pyroSAR import identify
    from pyroSAR.snap import geocode
    pyroSAR_available = True
except ImportError:
    pyroSAR = None
    identify = None
    geocode = None

try:
    from shapely.geometry import shape
    from shapely import wkt as shapely_wkt
    import geopandas as gpd
except ImportError as e:
    print(f"ERROR: No se pudo importar shapely/geopandas: {e}")
    print("Ejecuta: conda install -c conda-forge shapely geopandas")
    sys.exit(1)

# Configurar logging
os.makedirs('logs/preprocess', exist_ok=True)
log_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
log_file_path = f'logs/preprocess/preprocess_{log_timestamp}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
logger.info(f"Log file: {log_file_path}")


def load_config(config_file="config.txt"):
    """Cargar configuraci√≥n desde config.txt"""
    config = {
        'GRD_DIR': 'data/sentinel1_grd',
        'SLC_DIR': 'data/sentinel1_slc',
        'OUTPUT_DIR': 'processed',
        'AOI': 'POLYGON((2.405319 41.514032, 2.555008 41.514032, 2.555008 41.627029, 2.405319 41.627029, 2.405319 41.514032))'
    }

    if not os.path.exists(config_file):
        logger.warning(f"No se encontr√≥ {config_file}, usando valores por defecto")
        return config

    with open(config_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                config[key.strip()] = value.strip().strip('"')

    return config


def extract_date_from_filename(filename):
    """Extrae la fecha del nombre del archivo Sentinel-1"""
    match = re.search(r'(\d{8}T\d{6})', filename)
    if match:
        return match.group(1)
    return None


def extract_orbit_from_filename(filename):
    """Extrae el n√∫mero de √≥rbita relativa del nombre del archivo Sentinel-1"""
    # Formato: S1A_IW_GRDH_1SDV_20200101T123456_20200101T123521_030719_038A7E_1234.SAFE
    # El n√∫mero de √≥rbitas est√° en la posici√≥n 7 (030719 en este ejemplo)
    parts = filename.split('_')
    if len(parts) >= 7:
        try:
            # Los primeros 3 d√≠gitos del absolute orbit number dan el relative orbit
            orbit_abs = int(parts[6])
            # Para Sentinel-1, relative orbit = (absolute orbit - 73) % 175 + 1
            orbit_rel = ((orbit_abs - 73) % 175) + 1
            return orbit_rel
        except (ValueError, IndexError):
            pass
    return None


def group_products_by_date_orbit(products):
    """
    Agrupa productos por fecha y √≥rbita para identificar candidatos a mosaico

    Returns:
        dict: {(date, orbit): [list of product paths]}
    """
    groups = defaultdict(list)

    for product in products:
        basename = os.path.basename(product)
        date_str = extract_date_from_filename(basename)
        orbit = extract_orbit_from_filename(basename)

        if date_str and orbit:
            # Usar solo la fecha (YYYYMMDD), ignorar tiempo
            date_key = date_str[:8]
            groups[(date_key, orbit)].append(product)
        else:
            logger.warning(f"No se pudo extraer fecha/√≥rbita de: {basename}")

    return groups


def wkt_to_shapefile(wkt_string, output_shapefile):
    """
    Convierte un WKT polygon a shapefile

    Args:
        wkt_string: String WKT del pol√≠gono
        output_shapefile: Ruta de salida para el shapefile

    Returns:
        str: Ruta al shapefile creado o None si falla
    """
    try:
        # Parsear WKT
        geom = shapely_wkt.loads(wkt_string)

        # Crear GeoDataFrame
        gdf = gpd.GeoDataFrame({'geometry': [geom]}, crs='EPSG:4326')

        # Guardar como shapefile
        gdf.to_file(output_shapefile)

        logger.debug(f"  ‚úì Shapefile creado: {output_shapefile}")
        return output_shapefile

    except Exception as e:
        logger.error(f"  ‚úó Error creando shapefile: {e}")
        return None


def identify_product(product_path):
    """
    Identifica un producto Sentinel-1 usando PyroSAR

    Returns:
        pyroSAR.drivers.ID object or None
    """
    try:
        logger.debug(f"Identificando producto: {os.path.basename(product_path)}")
        product_id = identify(product_path)

        if product_id:
            logger.debug(f"  Sat√©lite: {product_id.sensor}")
            logger.debug(f"  Modo: {product_id.acquisition_mode}")
            logger.debug(f"  Producto: {product_id.product}")
            logger.debug(f"  Fecha: {product_id.start}")
            logger.debug(f"  √ìrbita: {product_id.orbit}")
            return product_id
        else:
            logger.warning(f"  No se pudo identificar el producto")
            return None

    except Exception as e:
        logger.error(f"  Error identificando producto: {e}")
        return None


def check_burst_coverage(product_path, aoi_wkt):
    """
    Verifica qu√© porcentaje del AOI cubre un burst
    
    Args:
        product_path: Ruta al producto .SAFE
        aoi_wkt: AOI en formato WKT
        
    Returns:
        float: Porcentaje de cobertura del AOI (0-100)
    """
    import xml.etree.ElementTree as ET
    from shapely import wkt
    from shapely.geometry import Polygon
    
    try:
        # Leer manifest del producto
        manifest_path = os.path.join(product_path, 'manifest.safe')
        if not os.path.exists(manifest_path):
            logger.warning(f"  No existe manifest: {manifest_path}")
            return 0.0
            
        tree = ET.parse(manifest_path)
        root = tree.getroot()
        
        # Buscar footprint del producto
        ns = {'gml': 'http://www.opengis.net/gml'}
        coords_elem = root.find('.//{http://www.opengis.net/gml}coordinates')
        
        if coords_elem is None:
            logger.warning(f"  No se encontr√≥ footprint en manifest")
            return 0.0
            
        # Parsear coordenadas (formato: lat1,lon1 lat2,lon2 ...)
        coords_text = coords_elem.text.strip()
        coord_pairs = coords_text.split()
        
        points = []
        for pair in coord_pairs:
            parts = pair.split(',')
            if len(parts) == 2:
                lat, lon = float(parts[0]), float(parts[1])
                points.append((lon, lat))  # Shapely usa (lon, lat)
        
        if len(points) < 3:
            logger.warning(f"  Footprint inv√°lido (menos de 3 puntos)")
            return 0.0
            
        # Crear pol√≠gonos
        burst_poly = Polygon(points)
        aoi_poly = wkt.loads(aoi_wkt)
        
        # Calcular intersecci√≥n
        intersection = burst_poly.intersection(aoi_poly)
        coverage = (intersection.area / aoi_poly.area) * 100
        
        return coverage
        
    except Exception as e:
        logger.error(f"  Error calculando cobertura: {e}")
        return 0.0


def detect_best_subswath_for_aoi(product_path, aoi_wkt):
    """
    Detecta qu√© sub-swath (IW1, IW2, IW3) contiene mejor el AOI

    Para productos SLC, cada sub-swath cubre una franja diferente.
    Esta funci√≥n analiza el annotation de cada sub-swath para determinar
    cu√°l intersecta mejor con el AOI.

    Args:
        product_path: Ruta al producto .SAFE
        aoi_wkt: AOI en formato WKT

    Returns:
        str: 'IW1', 'IW2' o 'IW3' - el sub-swath con mejor cobertura
    """
    # Usar la nueva funci√≥n multi-subswath y retornar el mejor
    subswaths = detect_all_subswaths_for_aoi(product_path, aoi_wkt)
    if subswaths:
        # Retornar el primero (ordenados por cobertura descendente)
        return subswaths[0][0]
    else:
        logger.warning(f"    No se pudo detectar sub-swath, usando IW1 por defecto")
        return 'IW1'


def detect_all_subswaths_for_aoi(product_path, aoi_wkt, min_coverage=5.0):
    """
    NUEVO: Detecta TODOS los sub-swaths que intersectan con el AOI

    MODIFICACI√ìN PARA DETECCI√ìN DE HUMEDAD:
    En lugar de seleccionar solo el "mejor" subswath, esta funci√≥n retorna
    TODOS los que tienen cobertura >= min_coverage. Esto evita "puntos ciegos"
    cuando un pueblo est√° en la frontera entre subswaths.

    Args:
        product_path: Ruta al producto .SAFE
        aoi_wkt: AOI en formato WKT
        min_coverage: Cobertura m√≠nima (%) para considerar un subswath

    Returns:
        list: Lista de tuplas [(subswath, coverage), ...] ordenadas por cobertura desc
              Solo incluye subswaths con cobertura >= min_coverage
              Ej: [('IW2', 85.3), ('IW1', 45.2)]
    """
    import xml.etree.ElementTree as ET
    from shapely import wkt
    from shapely.geometry import box

    try:
        aoi_poly = wkt.loads(aoi_wkt)

        # Buscar archivos de annotation para cada sub-swath
        annotation_dir = os.path.join(product_path, 'annotation')

        if not os.path.isdir(annotation_dir):
            logger.warning(f"  No existe directorio annotation en {product_path}")
            return [('IW1', 0.0)]  # Default fallback

        # Lista para almacenar todos los subswaths con cobertura
        subswath_coverages = []

        # Solo IW1 e IW2 (IW3 no recomendado para InSAR)
        for subswath in ['IW1', 'IW2']:
            # Buscar archivo annotation del sub-swath
            import glob
            pattern = os.path.join(annotation_dir, f's1*-{subswath.lower()}-slc-*.xml')
            annotation_files = glob.glob(pattern)

            if not annotation_files:
                continue

            annotation_file = annotation_files[0]

            try:
                tree = ET.parse(annotation_file)
                root = tree.getroot()

                # Extraer coordenadas del sub-swath desde geolocationGrid
                lat_values = []
                lon_values = []

                for geolocation_point in root.findall('.//geolocationGridPoint'):
                    lat_elem = geolocation_point.find('latitude')
                    lon_elem = geolocation_point.find('longitude')
                    if lat_elem is not None and lon_elem is not None:
                        lat_values.append(float(lat_elem.text))
                        lon_values.append(float(lon_elem.text))

                if lat_values and lon_values:
                    # Crear bounding box del sub-swath
                    swath_bounds = (
                        min(lon_values),  # minx
                        min(lat_values),  # miny
                        max(lon_values),  # maxx
                        max(lat_values)   # maxy
                    )

                    swath_box = box(*swath_bounds)

                    # Calcular intersecci√≥n con AOI
                    intersection = swath_box.intersection(aoi_poly)
                    coverage = (intersection.area / aoi_poly.area) * 100 if intersection.area > 0 else 0.0

                    logger.debug(f"    {subswath}: cobertura {coverage:.1f}%")

                    # MODIFICADO: Agregar a lista si cumple cobertura m√≠nima
                    if coverage >= min_coverage:
                        subswath_coverages.append((subswath, coverage))

            except Exception as e:
                logger.debug(f"    Error analizando {subswath}: {e}")
                continue

        # Ordenar por cobertura descendente
        subswath_coverages.sort(key=lambda x: x[1], reverse=True)

        if subswath_coverages:
            logger.info(f"    ‚Üí Sub-swaths detectados: {[(s, f'{c:.1f}%') for s, c in subswath_coverages]}")
            if len(subswath_coverages) > 1:
                total_coverage = sum(c for _, c in subswath_coverages)
                logger.info(f"    ‚Üí Cobertura combinada: {min(total_coverage, 100):.1f}%")
                logger.info(f"    ‚ö†Ô∏è  AOI en frontera de subswaths - considerar procesar ambos")
            return subswath_coverages
        else:
            logger.warning(f"    No se encontraron sub-swaths con cobertura >= {min_coverage}%")
            return [('IW1', 0.0)]

    except Exception as e:
        logger.error(f"  Error detectando sub-swaths: {e}")
        return [('IW1', 0.0)]  # Default fallback


def get_burst_indices_for_aoi(product_path, subswath, aoi_wkt):
    """
    NUEVO: Calcula los √≠ndices de burst (firstBurstIndex, lastBurstIndex) que
    intersectan con el AOI.

    OPTIMIZACI√ìN PARA DETECCI√ìN DE HUMEDAD:
    En lugar de procesar toda la faja del subswath, esta funci√≥n identifica
    solo los bursts necesarios. Esto:
    - Reduce tiempo de procesamiento
    - Reduce uso de memoria
    - Evita procesar datos innecesarios

    IMPORTANTE: Se mantiene la prohibici√≥n del operador Subset geogr√°fico
    ANTES del Back-Geocoding para InSAR.

    Args:
        product_path: Ruta al producto .SAFE
        subswath: Sub-swath a analizar ('IW1', 'IW2', 'IW3')
        aoi_wkt: AOI en formato WKT

    Returns:
        tuple: (firstBurstIndex, lastBurstIndex) - √≠ndices 1-based para SNAP
               o (None, None) si no se pueden determinar
    """
    import xml.etree.ElementTree as ET
    from shapely import wkt
    from shapely.geometry import Polygon

    try:
        aoi_poly = wkt.loads(aoi_wkt)

        # Buscar archivo annotation del sub-swath
        annotation_dir = os.path.join(product_path, 'annotation')
        import glob
        pattern = os.path.join(annotation_dir, f's1*-{subswath.lower()}-slc-*.xml')
        annotation_files = glob.glob(pattern)

        if not annotation_files:
            logger.warning(f"  No se encontr√≥ annotation para {subswath}")
            return (None, None)

        annotation_file = annotation_files[0]
        tree = ET.parse(annotation_file)
        root = tree.getroot()

        # Extraer informaci√≥n de geolocalizaci√≥n por burst
        # Cada burst tiene m√∫ltiples puntos en geolocationGrid
        geolocation_points = root.findall('.//geolocationGridPoint')

        if not geolocation_points:
            logger.warning(f"  No se encontraron puntos de geolocalizaci√≥n")
            return (None, None)

        # Agrupar puntos por l√≠nea azimuth (cada burst tiene ~21 l√≠neas)
        # El atributo 'line' indica la l√≠nea dentro del burst
        burst_lines = {}
        for point in geolocation_points:
            azimuth_time = point.find('azimuthTime')
            lat = point.find('latitude')
            lon = point.find('longitude')
            line_elem = point.find('line')

            if all(elem is not None for elem in [lat, lon, line_elem]):
                line_num = int(line_elem.text)
                if line_num not in burst_lines:
                    burst_lines[line_num] = []
                burst_lines[line_num].append((float(lon.text), float(lat.text)))

        # Obtener n√∫mero de bursts desde swathTiming
        swath_timing = root.find('.//swathTiming')
        if swath_timing is not None:
            burst_list = swath_timing.find('burstList')
            if burst_list is not None:
                num_bursts = int(burst_list.get('count', 0))
                logger.debug(f"    {subswath}: {num_bursts} bursts")

        # Por ahora, sin informaci√≥n detallada de burst footprints,
        # retornamos None para usar todos los bursts
        # (implementaci√≥n completa requerir√≠a parsear burstList)
        logger.debug(f"    Selecci√≥n de bursts: usando todos los bursts del subswath")
        return (None, None)

    except Exception as e:
        logger.warning(f"  Error calculando burst indices: {e}")
        return (None, None)




def select_best_burst_for_aoi(products, aoi_wkt, min_coverage=1.0):
    """
    Selecciona el mejor burst para cubrir el AOI
    
    Args:
        products: Lista de rutas a productos .SAFE
        aoi_wkt: AOI en formato WKT
        min_coverage: Cobertura m√≠nima requerida (%)
        
    Returns:
        str: Ruta al mejor producto o None
    """
    if not products:
        return None
        
    if len(products) == 1:
        # Solo un burst, verificar cobertura
        coverage = check_burst_coverage(products[0], aoi_wkt)
        logger.info(f"    Un solo burst disponible: cobertura {coverage:.1f}%")
        if coverage >= min_coverage:
            return products[0]
        else:
            logger.warning(f"    Cobertura insuficiente ({coverage:.1f}% < {min_coverage}%)")
            return products[0]  # Devolver de todas formas
    
    # M√∫ltiples bursts: seleccionar el de mejor cobertura
    best_product = None
    best_coverage = 0.0
    
    logger.info(f"    Evaluando {len(products)} bursts:")
    for product in products:
        basename = os.path.basename(product)
        coverage = check_burst_coverage(product, aoi_wkt)
        logger.info(f"      - {basename[:60]}: {coverage:.1f}% cobertura")
        
        if coverage > best_coverage:
            best_coverage = coverage
            best_product = product
    
    if best_coverage >= min_coverage:
        logger.info(f"    ‚úì Seleccionado: {os.path.basename(best_product)[:60]} ({best_coverage:.1f}%)")
    else:
        logger.warning(f"    ‚ö† Mejor cobertura: {best_coverage:.1f}% < {min_coverage}%")
        logger.warning(f"    Se necesitar√≠a fusi√≥n de bursts para cobertura completa")
    
    return best_product


def subset_product(product_path, aoi_wkt, output_dir, insar_mode=False, subswath=None):
    """
    Recorta un producto al AOI usando SNAP/GPT directamente (solo subset, sin procesamiento)

    Args:
        product_path: Ruta al producto .SAFE
        aoi_wkt: AOI en formato WKT (opcional en modo InSAR sin AOI)
        output_dir: Directorio de salida para producto recortado
        insar_mode: Si True, no aplica TOPSAR-Deburst (mantiene estructura de bursts para InSAR)
        subswath: Subswath espec√≠fico (IW1/IW2/IW3) para modo InSAR sin AOI

    Returns:
        str: Ruta al producto recortado o None si falla
    """
    import subprocess
    import tempfile

    try:
        basename = os.path.basename(product_path)
        date_str = extract_date_from_filename(basename)

        # Determinar tipo de producto (GRD o SLC)
        product_type = 'GRD' if '_GRDH_' in basename or '_GRDM_' in basename else 'SLC'

        # Nombre de salida
        # Usar el basename (sin extensi√≥n) para evitar colisiones cuando hay varios
        # productos del mismo d√≠a (por ejemplo, varios SLC del mismo date).
        base_noext = os.path.splitext(basename)[0]
        suffix = '_split' if insar_mode else '_subset'
        output_name = f"{product_type}_{date_str[:8]}_{base_noext}{suffix}"
        output_path = os.path.join(output_dir, output_name + '.dim')

        # Verificar si ya existe
        if os.path.exists(output_path):
            logger.info(f"  ‚úì Ya existe: {output_name}")
            return output_path

        if insar_mode:
            logger.info(f"  Procesando producto en modo InSAR (TOPSAR-Split)...")
        else:
            logger.info(f"  Recortando producto al AOI con SNAP...")
        logger.info(f"  ‚Üí Salida: {output_path}")

        # Crear XML seg√∫n tipo de producto
        if product_type == 'SLC':
            # Detectar subswath
            if subswath:
                # Usar subswath especificado (modo InSAR sin AOI)
                # RESTRICCI√ìN: Solo IW1 o IW2
                if subswath.upper() not in ['IW1', 'IW2']:
                    logger.warning(f"  ‚ö†Ô∏è  Subswath {subswath} no v√°lido para InSAR, usando IW1")
                    best_subswath = "IW1"
                else:
                    best_subswath = subswath
            elif aoi_wkt:
                # Detectar qu√© sub-swath contiene el AOI
                detected = detect_best_subswath_for_aoi(product_path, aoi_wkt)
                # RESTRICCI√ìN: Solo IW1 o IW2
                if detected == "IW3":
                    logger.warning("  ‚ö†Ô∏è  AOI en IW3, usando IW2 en su lugar (IW3 no recomendado para InSAR)")
                    best_subswath = "IW2"
                else:
                    best_subswath = detected
            else:
                # Por defecto IW1 (preferencia sobre IW2)
                logger.warning("  ‚ö†Ô∏è  Sin AOI ni subswath especificado, usando IW1")
                best_subswath = "IW1"
            
            if insar_mode:
                # Para InSAR: TOPSAR-Split (SIN Deburst, SIN Subset)
                # Mantener estructura de bursts completa para Back-Geocoding
                # El Subset se har√° al final del workflow InSAR despu√©s de Terrain-Correction
                subset_xml = f"""<graph id="SLC_Split_InSAR">
  <version>1.0</version>

  <node id="Read">
    <operator>Read</operator>
    <sources/>
    <parameters>
      <file>{product_path}</file>
    </parameters>
  </node>

  <node id="TOPSAR-Split">
    <operator>TOPSAR-Split</operator>
    <sources>
      <sourceProduct refid="Read"/>
    </sources>
    <parameters>
      <subswath>{best_subswath}</subswath>
      <selectedPolarisations>VV</selectedPolarisations>
    </parameters>
  </node>

  <node id="Write">
    <operator>Write</operator>
    <sources>
      <sourceProduct refid="TOPSAR-Split"/>
    </sources>
    <parameters>
      <file>{output_path}</file>
      <formatName>BEAM-DIMAP</formatName>
    </parameters>
  </node>

</graph>"""
            else:
                # Para SAR normal: TOPSAR-Split + TOPSAR-Deburst + Subset
                # IMPORTANTE: Deburst ANTES de Subset para evitar problemas de geometr√≠a
                subset_xml = f"""<graph id="SLC_Split_Deburst_Subset">
  <version>1.0</version>

  <node id="Read">
    <operator>Read</operator>
    <sources/>
    <parameters>
      <file>{product_path}</file>
    </parameters>
  </node>

  <node id="TOPSAR-Split">
    <operator>TOPSAR-Split</operator>
    <sources>
      <sourceProduct refid="Read"/>
    </sources>
    <parameters>
      <subswath>{best_subswath}</subswath>
      <selectedPolarisations>VV</selectedPolarisations>
    </parameters>
  </node>

  <node id="TOPSAR-Deburst">
    <operator>TOPSAR-Deburst</operator>
    <sources>
      <sourceProduct refid="TOPSAR-Split"/>
    </sources>
    <parameters>
      <selectedPolarisations>VV</selectedPolarisations>
    </parameters>
  </node>

  <node id="Subset">
    <operator>Subset</operator>
    <sources>
      <sourceProduct refid="TOPSAR-Deburst"/>
    </sources>
    <parameters>
      <geoRegion>{aoi_wkt}</geoRegion>
      <copyMetadata>true</copyMetadata>
    </parameters>
  </node>

  <node id="Write">
    <operator>Write</operator>
    <sources>
      <sourceProduct refid="Subset"/>
    </sources>
    <parameters>
      <file>{output_path}</file>
      <formatName>BEAM-DIMAP</formatName>
    </parameters>
  </node>

</graph>"""
        else:
            # Para GRD: usar Subset directo
            subset_xml = f"""<graph id="GRD_Subset">
  <version>1.0</version>

  <node id="Read">
    <operator>Read</operator>
    <sources/>
    <parameters>
      <file>{product_path}</file>
    </parameters>
  </node>

  <node id="Subset">
    <operator>Subset</operator>
    <sources>
      <sourceProduct refid="Read"/>
    </sources>
    <parameters>
      <geoRegion>{aoi_wkt}</geoRegion>
      <copyMetadata>true</copyMetadata>
      <fullSwath>false</fullSwath>
    </parameters>
  </node>

  <node id="Write">
    <operator>Write</operator>
    <sources>
      <sourceProduct refid="Subset"/>
    </sources>
    <parameters>
      <file>{output_path}</file>
      <formatName>BEAM-DIMAP</formatName>
    </parameters>
  </node>

</graph>"""

        # Guardar XML temporal
        with tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False) as tf:
            tf.write(subset_xml)
            xml_file = tf.name

        try:
            # Ejecutar GPT
            logger.info(f"  ‚öôÔ∏è  Ejecutando GPT para subset...")
            result = subprocess.run(
                ['gpt', xml_file, '-c', '4G'],
                capture_output=True,
                text=True,
                timeout=600  # 10 minutos timeout
            )

            # Verificar que el archivo se cre√≥ correctamente
            if os.path.exists(output_path):
                logger.info(f"  ‚úì Subset creado: {output_path}")
                return output_path
            else:
                logger.error(f"  ‚úó Error: GPT termin√≥ pero no cre√≥ el archivo (exit code {result.returncode})")
                if result.stdout:
                    logger.error(f"STDOUT: {result.stdout[-1000:]}")
                if result.stderr:
                    logger.error(f"STDERR: {result.stderr[-1000:]}")
                return None

        finally:
            # Limpiar XML temporal
            if os.path.exists(xml_file):
                os.unlink(xml_file)

    except Exception as e:
        logger.error(f"  ‚úó Error recortando producto: {e}")
        logger.exception("Detalles:")
        return None


def create_mosaic(product_list, aoi_wkt, output_dir):
    """
    Crea un mosaico de m√∫ltiples productos Sentinel-1

    Args:
        product_list: Lista de rutas a productos .SAFE
        aoi_wkt: AOI en formato WKT
        output_dir: Directorio de salida

    Returns:
        str: Ruta al mosaico o None si falla
    """
    try:
        if len(product_list) < 2:
            logger.warning("  Se necesitan al menos 2 productos para crear mosaico")
            return None

        # Extraer informaci√≥n del primer producto
        basename = os.path.basename(product_list[0])
        date_str = extract_date_from_filename(basename)
        orbit = extract_orbit_from_filename(basename)
        product_type = 'GRD' if '_GRDH_' in basename or '_GRDM_' in basename else 'SLC'

        mosaic_name = f"{product_type}_{date_str[:8]}_orbit{orbit:03d}_mosaic"
        output_path = os.path.join(output_dir, mosaic_name + '.dim')

        # Verificar si ya existe
        if os.path.exists(output_path):
            logger.info(f"  ‚úì Ya existe mosaico: {mosaic_name}")
            return output_path

        logger.info(f"  Creando mosaico de {len(product_list)} productos...")
        logger.info(f"  ‚Üí Productos: {[os.path.basename(p)[:40] + '...' for p in product_list]}")
        logger.info(f"  ‚Üí Salida: {output_path}")

        # PyroSAR no tiene funci√≥n directa de mosaico, tendr√≠amos que usar SNAP directamente
        # Por ahora, procesaremos productos individualmente
        logger.warning("  ‚ö†Ô∏è  Funcionalidad de mosaico no implementada a√∫n")
        logger.warning("  Los productos ser√°n procesados individualmente")

        return None

    except Exception as e:
        logger.error(f"  ‚úó Error creando mosaico: {e}")
        logger.exception("Detalles:")
        return None


def process_single_product(args_tuple):
    """
    Procesa un producto individual (funci√≥n helper para paralelizaci√≥n)

    Args:
        args_tuple: (index, total, product, aoi_wkt, output_dir, insar_mode, subswath_from_dir)

    Returns:
        tuple: (product_name, success)
    """
    index, total, product, aoi_wkt, output_dir, insar_mode, subswath_from_dir = args_tuple

    # Importar logger y funci√≥n subset_product localmente
    from processing_utils import logger, extract_date_from_filename
    import os

    product_name = os.path.basename(product)[:60]
    logger.info(f"[{index}/{total}] {product_name}...")

    result = subset_product(product, aoi_wkt, output_dir, insar_mode=insar_mode, subswath=subswath_from_dir)

    return (product_name, result)


def preprocess_products(product_dir, output_dir, aoi_wkt, product_type='GRD', create_mosaics=False, insar_mode=False):
    """
    Pre-procesa productos Sentinel-1

    Args:
        product_dir: Directorio con productos .SAFE
        output_dir: Directorio de salida
        aoi_wkt: AOI en formato WKT (puede ser None en modo InSAR)
        product_type: 'GRD' or 'SLC'
        create_mosaics: Si True, crea mosaicos de productos con misma fecha/√≥rbita
        insar_mode: Si True (solo SLC), no aplica Deburst (mantiene bursts para InSAR)
    """
    logger.info("=" * 80)
    logger.info(f"PRE-PROCESAMIENTO DE PRODUCTOS {product_type}")
    if product_type == 'SLC' and insar_mode:
        logger.info("MODO: InSAR (sin TOPSAR-Deburst, sin AOI)")
    elif product_type == 'SLC':
        logger.info("MODO: SAR Normal (con TOPSAR-Deburst)")
    logger.info("=" * 80)
    
    # Detectar subswath del nombre del directorio (para modo InSAR sin AOI)
    subswath_from_dir = None
    if insar_mode and not aoi_wkt:
        # Formato: data/preprocessed_slc_insar/desc_iw1/ ‚Üí IW1
        import re
        match = re.search(r'_(iw[123])$', os.path.basename(output_dir.rstrip('/')))
        if match:
            subswath_from_dir = match.group(1).upper()  # IW1, IW2, IW3
            logger.info(f"Subswath detectado del directorio: {subswath_from_dir}")

    # Buscar productos
    if product_type == 'GRD':
        pattern = os.path.join(product_dir, 'S1*_IW_GRDH_*.SAFE')
    else:  # SLC
        pattern = os.path.join(product_dir, 'S1*_IW_SLC__*.SAFE')

    products = sorted(glob.glob(pattern))

    if not products:
        logger.warning(f"No se encontraron productos {product_type} en {product_dir}")
        return

    logger.info(f"Encontrados {len(products)} productos {product_type}")

    # Filtrar COG si es GRD
    if product_type == 'GRD':
        non_cog = [p for p in products if '_COG.SAFE' not in os.path.basename(p)]
        if len(non_cog) < len(products):
            logger.info(f"  Omitiendo {len(products) - len(non_cog)} productos COG")
            products = non_cog

    logger.info(f"  {len(products)} productos para pre-procesar")

    # Crear directorio de salida
    os.makedirs(output_dir, exist_ok=True)

    # Para SLC: agrupar por fecha y seleccionar mejor burst
    # (no crear mosaicos, pero s√≠ seleccionar el burst con mejor cobertura)
    if product_type == 'SLC':
        logger.info('\nPara SLC: agrupando por fecha y seleccionando mejor burst por cobertura AOI...')
        create_mosaics = False
        
        # Agrupar por fecha (ignorar √≥rbita)
        by_date = defaultdict(list)
        for product in products:
            basename = os.path.basename(product)
            date_str = extract_date_from_filename(basename)
            if date_str:
                date_key = date_str[:8]
                by_date[date_key].append(product)
        
        logger.info(f'  Fechas √∫nicas: {len(by_date)}')
        
        # Seleccionar mejor burst por fecha
        selected_products = []
        for date, date_products in sorted(by_date.items()):
            if len(date_products) > 1:
                logger.info(f'\n  Fecha {date}: {len(date_products)} slices detectados (Frontera Azimutal)')
                # A√±adir TODOS los que tengan cobertura m√≠nima (> 1%)
                for p in date_products:
                    cov = check_burst_coverage(p, aoi_wkt)
                    if cov > 1.0:
                        logger.info(f"    -> Agregando slice con cobertura {cov:.1f}%")
                        selected_products.append(p)
                    else:
                        logger.info(f"    -> Descartando slice con cobertura {cov:.1f}% (< 1%)")
            else:
                # L√≥gica para un solo producto (mantiene compatibilidad)
                logger.info(f'\n  Fecha {date}: 1 producto')
                best = select_best_burst_for_aoi(date_products, aoi_wkt, min_coverage=1.0)
                if best:
                    selected_products.append(best)
        
        logger.info(f'\n  Productos seleccionados: {len(selected_products)}')
        products = selected_products
    
    # Agrupar por fecha/√≥rbita para mosaicos (solo GRD)
    if create_mosaics:
        groups = group_products_by_date_orbit(products)
        logger.info(f"\nAgrupaci√≥n para mosaicos:")
        logger.info(f"  Total de grupos (fecha+√≥rbita): {len(groups)}")

        mosaic_count = 0
        single_count = 0

        for (date, orbit), group_products in groups.items():
            if len(group_products) > 1:
                mosaic_count += 1
                logger.info(f"  {date} √≥rbita {orbit:03d}: {len(group_products)} productos ‚Üí MOSAICO")
            else:
                single_count += 1

        logger.info(f"  Grupos con mosaico: {mosaic_count}")
        logger.info(f"  Productos individuales: {single_count}")
        logger.info("")

        # Procesar cada grupo
        processed = 0
        failed = 0

        for (date, orbit), group_products in groups.items():
            logger.info(f"[{date} √≥rbita {orbit:03d}]")

            if len(group_products) > 1:
                # Crear mosaico
                result = create_mosaic(group_products, aoi_wkt, output_dir)
                if result:
                    processed += 1
                else:
                    # Si falla el mosaico, procesar individualmente EN PARALELO
                    logger.info("  Procesando productos individualmente (en paralelo)...")

                    # Preparar tareas para este grupo
                    group_tasks = [
                        (i, len(group_products), product, aoi_wkt, output_dir, insar_mode, subswath_from_dir)
                        for i, product in enumerate(group_products, 1)
                    ]

                    with ProcessPoolExecutor(max_workers=3) as executor:
                        future_to_task = {
                            executor.submit(process_single_product, task): task
                            for task in group_tasks
                        }

                        for future in as_completed(future_to_task):
                            try:
                                product_name, result = future.result()
                                if result:
                                    processed += 1
                                else:
                                    failed += 1
                            except Exception as e:
                                logger.error(f"  ‚ùå Excepci√≥n: {e}")
                                failed += 1
            else:
                # Producto individual
                result = subset_product(group_products[0], aoi_wkt, output_dir, insar_mode=insar_mode, subswath=subswath_from_dir)
                if result:
                    processed += 1
                else:
                    failed += 1

    else:
        # Procesar todos individualmente EN PARALELO (3 workers)
        logger.info("\nProcesando productos individualmente (sin mosaicos)...")
        logger.info(f"üöÄ Usando 3 workers en paralelo...")
        logger.info("")

        processed = 0
        failed = 0

        # Preparar argumentos para paralelizaci√≥n
        tasks = [
            (i, len(products), product, aoi_wkt, output_dir, insar_mode, subswath_from_dir)
            for i, product in enumerate(products, 1)
        ]

        # Procesar en paralelo con ProcessPoolExecutor
        with ProcessPoolExecutor(max_workers=3) as executor:
            # Enviar todas las tareas
            future_to_task = {
                executor.submit(process_single_product, task): task
                for task in tasks
            }

            # Recolectar resultados conforme se completan
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    product_name, result = future.result()

                    if result:
                        processed += 1
                    else:
                        failed += 1

                except Exception as e:
                    logger.error(f"  ‚ùå Excepci√≥n procesando producto: {e}")
                    failed += 1

    # Resumen
    logger.info("")
    logger.info("=" * 80)
    logger.info("RESUMEN PRE-PROCESAMIENTO")
    logger.info("=" * 80)
    logger.info(f"Productos procesados correctamente: {processed}")
    logger.info(f"Productos fallidos: {failed}")
    logger.info(f"Total: {len(products)}")
    logger.info(f"\nProductos pre-procesados guardados en: {output_dir}")
    logger.info("")


def main():
    parser = argparse.ArgumentParser(
        description='Pre-procesa productos Sentinel-1 con PyroSAR',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  # Pre-procesar productos GRD
  python scripts/preprocess_products.py --grd

  # Pre-procesar productos SLC (modo SAR normal, con Deburst)
  python scripts/preprocess_products.py --slc

  # Pre-procesar productos SLC para InSAR (sin Deburst)
  python scripts/preprocess_products.py --slc --insar-mode

  # Pre-procesar con mosaicos
  python scripts/preprocess_products.py --grd --mosaic

  # Pre-procesar ambos tipos
  python scripts/preprocess_products.py --grd --slc
        """
    )

    parser.add_argument('--grd', action='store_true', help='Pre-procesar productos GRD')
    parser.add_argument('--slc', action='store_true', help='Pre-procesar productos SLC')
    parser.add_argument('--mosaic', action='store_true', help='Crear mosaicos de productos con misma fecha/√≥rbita')
    parser.add_argument('--insar-mode', action='store_true', help='Modo InSAR para SLC (sin Deburst, mantiene estructura de bursts)')
    parser.add_argument('--config', type=str, default='config.txt', help='Archivo de configuraci√≥n (default: config.txt)')

    args = parser.parse_args()

    # Si no se especifica ninguno, procesar ambos
    if not args.grd and not args.slc:
        args.grd = True
        args.slc = True

    logger.info("=" * 80)
    logger.info("PRE-PROCESAMIENTO SENTINEL-1")
    logger.info("=" * 80)
    if pyroSAR_available:
        logger.info(f"PyroSAR versi√≥n: {pyroSAR.__version__}")
    else:
        logger.info("PyroSAR: No disponible (usando solo SNAP/GPT)")
    logger.info("")

    # Cargar configuraci√≥n
    config = load_config(args.config)
    aoi = config.get('AOI')

    logger.info(f"Configuraci√≥n:")
    logger.info(f"  AOI: {aoi[:60]}...")
    logger.info(f"  Crear mosaicos: {'S√≠' if args.mosaic else 'No'}")
    if args.slc:
        logger.info(f"  Modo InSAR (SLC sin Deburst): {'S√≠' if args.insar_mode else 'No'}")
    logger.info("")

    # Pre-procesar GRD
    if args.grd:
        grd_dir = config.get('GRD_DIR', 'data/sentinel1_grd')
        grd_output = config.get('PREPROCESSED_GRD_DIR', 'data/preprocessed_grd')

        logger.info(f"  Directorio GRD entrada: {grd_dir}")
        logger.info(f"  Directorio GRD salida: {grd_output}")
        
        if os.path.isdir(grd_dir):
            preprocess_products(grd_dir, grd_output, aoi, product_type='GRD', create_mosaics=args.mosaic)
        else:
            logger.warning(f"Directorio GRD no existe: {grd_dir}")

    # Pre-procesar SLC
    if args.slc:
        slc_dir = config.get('SLC_DIR', 'data/sentinel1_slc')
        slc_output = config.get('PREPROCESSED_SLC_DIR', 'data/preprocessed_slc')

        logger.info(f"  Directorio SLC entrada: {slc_dir}")
        logger.info(f"  Directorio SLC salida: {slc_output}")
        
        if os.path.isdir(slc_dir):
            preprocess_products(slc_dir, slc_output, aoi, product_type='SLC', 
                              create_mosaics=args.mosaic, insar_mode=args.insar_mode)
        else:
            logger.warning(f"Directorio SLC no existe: {slc_dir}")

    logger.info("=" * 80)
    logger.info("¬°PRE-PROCESAMIENTO COMPLETADO!")
    logger.info("=" * 80)

    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        logger.warning("\n\nInterrumpido por el usuario")
        sys.exit(130)
    except Exception as e:
        logger.error(f"\nERROR: {str(e)}", exc_info=True)
        sys.exit(1)
